{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "Sm33j3Ie-reX",
    "outputId": "1f499bf7-c914-446e-8e82-f728be502f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\projects\\car\\train\n",
      "D:\\projects\\car\\val\n",
      "D:\\projects\\car\\test\n",
      "beige D:\\projects\\car\\train\n",
      "black D:\\projects\\car\\train\n",
      "blue D:\\projects\\car\\train\n",
      "brown D:\\projects\\car\\train\n",
      "gold D:\\projects\\car\\train\n",
      "green D:\\projects\\car\\train\n",
      "grey D:\\projects\\car\\train\n",
      "orange D:\\projects\\car\\train\n",
      "pink D:\\projects\\car\\train\n",
      "purple D:\\projects\\car\\train\n",
      "red D:\\projects\\car\\train\n",
      "silver D:\\projects\\car\\train\n",
      "tan D:\\projects\\car\\train\n",
      "white D:\\projects\\car\\train\n",
      "yellow D:\\projects\\car\\train\n",
      "beige D:\\projects\\car\\val\n",
      "black D:\\projects\\car\\val\n",
      "blue D:\\projects\\car\\val\n",
      "brown D:\\projects\\car\\val\n",
      "gold D:\\projects\\car\\val\n",
      "green D:\\projects\\car\\val\n",
      "grey D:\\projects\\car\\val\n",
      "orange D:\\projects\\car\\val\n",
      "pink D:\\projects\\car\\val\n",
      "purple D:\\projects\\car\\val\n",
      "red D:\\projects\\car\\val\n",
      "silver D:\\projects\\car\\val\n",
      "tan D:\\projects\\car\\val\n",
      "white D:\\projects\\car\\val\n",
      "yellow D:\\projects\\car\\val\n",
      "beige D:\\projects\\car\\test\n",
      "black D:\\projects\\car\\test\n",
      "blue D:\\projects\\car\\test\n",
      "brown D:\\projects\\car\\test\n",
      "gold D:\\projects\\car\\test\n",
      "green D:\\projects\\car\\test\n",
      "grey D:\\projects\\car\\test\n",
      "orange D:\\projects\\car\\test\n",
      "pink D:\\projects\\car\\test\n",
      "purple D:\\projects\\car\\test\n",
      "red D:\\projects\\car\\test\n",
      "silver D:\\projects\\car\\test\n",
      "tan D:\\projects\\car\\test\n",
      "white D:\\projects\\car\\test\n",
      "yellow D:\\projects\\car\\test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Base directory where your datasets are located\n",
    "base_dir = 'D:\\\\projects\\\\car'\n",
    "\n",
    "# Specific directories for training, validation, and testing\n",
    "train_dir = 'D:\\\\projects\\\\car\\\\train'\n",
    "val_dir = 'D:\\\\projects\\\\car\\\\val'\n",
    "test_dir = 'D:\\\\projects\\\\car\\\\test'\n",
    "\n",
    "print(train_dir)\n",
    "print(val_dir)\n",
    "print(test_dir)\n",
    "# Load your dataset\n",
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label_dir in os.listdir(data_dir):\n",
    "        print(label_dir, data_dir)\n",
    "        label_path = os.path.join(data_dir, label_dir)\n",
    "        for image_name in os.listdir(label_path):\n",
    "            image_path = os.path.join(label_path, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "            images.append(image)\n",
    "            labels.append(label_dir)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load train, val, test datasets\n",
    "X_train, y_train = load_data(train_dir)\n",
    "X_val, y_val = load_data(val_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# Preprocess labels (one-hot encoding)\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_val = lb.transform(y_val)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\preyes\\anaconda3\\lib\\site-packages (24.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Preyes\\anaconda3\\python.exe -m pip install --upgrade pip setuptools wheel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pip\n",
      "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\preyes\\anaconda3\\lib\\site-packages (69.5.1)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-75.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: wheel in c:\\users\\preyes\\anaconda3\\lib\\site-packages (0.43.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "Downloading setuptools-75.0.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB 487.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.4/1.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 6.1 MB/s eta 0:00:00\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\preyes\\appdata\\roaming\\python\\python312\\site-packages (24.1)\n",
      "Files removed: 1693\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [44 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 35, in __init__\n",
      "      parsed = _parse_requirement(requirement_string)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 64, in parse_requirement\n",
      "      return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 82, in _parse_requirement\n",
      "      url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 126, in _parse_requirement_details\n",
      "      marker = _parse_requirement_marker(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 147, in _parse_requirement_marker\n",
      "      tokenizer.raise_syntax_error(\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_tokenizer.py\", line 165, in raise_syntax_error\n",
      "      raise ParserSyntaxError(\n",
      "  setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Preyes\\AppData\\Local\\Temp\\pip-install-1vryime6\\tensorflow-gpu_f2b3e2df3265413f9c4f020eab7901ba\\setup.py\", line 40, in <module>\n",
      "      setuptools.setup()\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 103, in setup\n",
      "      _install_setup_requires(attrs)\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\__init__.py\", line 74, in _install_setup_requires\n",
      "      dist.parse_config_files(ignore_option_errors=True)\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 633, in parse_config_files\n",
      "      self._finalize_requires()\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 368, in _finalize_requires\n",
      "      self._normalize_requires()\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\dist.py\", line 383, in _normalize_requires\n",
      "      self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 37, in __init__\n",
      "      raise InvalidRequirement(str(e)) from e\n",
      "  setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install --upgrade packaging\n",
    "!pip cache purge\n",
    "\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HtNMs8TDhjk",
    "outputId": "215a602d-bcf1-4c2e-de4e-584846354203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory: D:\\projects\\car\\train\n",
      "Val directory: D:\\projects\\car\\val\n",
      "Test directory: D:\\projects\\car\\test\n",
      "Train directory exists: True\n",
      "Val directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Val directory: {val_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Val directory exists: {os.path.exists(val_dir)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(test_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6iVp2iA1Dupi"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(512, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(lb.classes_), activation='softmax')\n",
    "\n",
    "        \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujD60ef4J3OR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m197/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3:38\u001b[0m 7s/step - accuracy: 0.0904 - loss: 104.1713"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=25, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1556 images belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Preyes\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 268ms/step - accuracy: 0.1812 - loss: 2.5788\n",
      "Test Loss: 2.7211854457855225\n",
      "Test Accuracy: 0.10025706887245178\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Load the pre-trained model\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Compile the model with an optimizer, loss function, and metrics (if not done already)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load the test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Normalization\n",
    "\n",
    "# Directory where your test dataset is stored (update the path accordingly)\n",
    "test_dir = 'test'\n",
    "\n",
    "\n",
    "\n",
    "# Load the test data using flow_from_directory\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),  # Change according to your model's input size\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # If multi-class classification\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# Display the results\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NZmje4a-J82s"
   },
   "outputs": [],
   "source": [
    "def detect_cars_and_people(image):\n",
    "    car_cascade = cv2.CascadeClassifier('cars.xml')  # Replace with the correct path to your car cascade file\n",
    "    person_cascade = cv2.CascadeClassifier('haarcascade_fullbody.xml')  # Replace with the correct path to your person cascade file\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cars = car_cascade.detectMultiScale(gray, 1.1, 1)\n",
    "    persons = person_cascade.detectMultiScale(gray, 1.1, 1)\n",
    "\n",
    "    car_count = 0\n",
    "    for (x, y, w, h) in cars:\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        car_image = cv2.resize(car_image, (224, 224))\n",
    "        car_image = np.expand_dims(car_image, axis=0)\n",
    "\n",
    "        prediction = model.predict(car_image)\n",
    "        color = lb.classes_[np.argmax(prediction)]\n",
    "\n",
    "        if color == 'blue':\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red rectangle\n",
    "        else:\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)  # Blue rectangle\n",
    "\n",
    "        car_count += 1\n",
    "\n",
    "    person_count = len(persons)\n",
    "    cv2.putText(image, f'Persons: {person_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    return image, car_count, person_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_9Ev9qpCKaZ8"
   },
   "outputs": [],
   "source": [
    "# Load the Haar Cascade classifiers\n",
    "car_cascade_path = r'D:\\projects\\car\\cars.xml'  \n",
    "person_cascade_path = r'D:\\projects\\car\\cars.xml'  \n",
    "car_cascade = cv2.CascadeClassifier(car_cascade_path)\n",
    "person_cascade = cv2.CascadeClassifier(person_cascade_path)\n",
    "\n",
    "# Function to detect cars and people and classify car colors\n",
    "def detect_cars_and_people(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect cars\n",
    "    cars = car_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))\n",
    "    \n",
    "    # Detect people\n",
    "    persons = person_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))\n",
    "    \n",
    "    car_count = 0\n",
    "    for (x, y, w, h) in cars:\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        car_image_resized = cv2.resize(car_image, (224, 224))\n",
    "        car_image_array = np.expand_dims(car_image_resized, axis=0)  # Prepare the image for the model\n",
    "        \n",
    "        # Predict car color\n",
    "        prediction = model.predict(car_image_array)\n",
    "        color = lb.classes_[np.argmax(prediction)]\n",
    "        \n",
    "        if color == 'blue':\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red rectangle for blue cars\n",
    "        else:\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)  # Blue rectangle for other colors\n",
    "        \n",
    "        car_count += 1\n",
    "\n",
    "    person_count = len(persons)\n",
    "    for (x, y, w, h) in persons:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green rectangle for detected people\n",
    "    \n",
    "    cv2.putText(image, f'Persons: {person_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    print(image,car_count,person_count)\n",
    "    return image, car_count, person_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('my_model.keras')\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Compile the model with the required metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# cars.xml =r'D:\\projects\\car\\cars.xml'\n",
    "# haarcascade_fullbody.xml = r'haarcascade_fullbody.xml'\n",
    "# Load Haar cascades\n",
    "car_cascade = cv2.CascadeClassifier('cars.xml')\n",
    "person_cascade = cv2.CascadeClassifier('haarcascade_fullbody.xml')\n",
    "\n",
    "# Label Binarizer classes (update these based on your trained classes)\n",
    "lb_classes = ['black', 'blue', 'brown', 'gold','green', 'grey', 'orange', 'pink','purple', 'red', 'silver', 'tan','white', 'yellow', 'beige']  \n",
    "global img, loaded_img\n",
    "\n",
    "# Define maximum window and image size\n",
    "MAX_WINDOW_WIDTH = 800\n",
    "MAX_WINDOW_HEIGHT = 600\n",
    "\n",
    "def resize_image(image, max_width, max_height):\n",
    "    \"\"\"Resizes the image to fit within the given max dimensions while maintaining aspect ratio.\"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    if width > max_width or height > max_height:\n",
    "        aspect_ratio = width / height\n",
    "        if width > height:\n",
    "            new_width = min(width, max_width)\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        else:\n",
    "            new_height = min(height, max_height)\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "        resized_image = cv2.resize(image, (new_width, new_height))\n",
    "    else:\n",
    "        resized_image = image\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "def detect_cars_and_people(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect cars\n",
    "    cars = car_cascade.detectMultiScale(gray, 1.1, 3)\n",
    "    \n",
    "    # Detect people\n",
    "    persons = person_cascade.detectMultiScale(gray, 1.1, 3)\n",
    "    \n",
    "    # Process detected cars\n",
    "    for (x, y, w, h) in cars:\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        car_image_resized = cv2.resize(car_image, (224, 224))\n",
    "        car_image_array = np.expand_dims(car_image_resized, axis=0)\n",
    "        \n",
    "        # Predict the car color\n",
    "        prediction = model.predict(car_image_array)\n",
    "        color = lb_classes[np.argmax(prediction)]\n",
    "        \n",
    "        # Draw bounding boxes based on the car color\n",
    "        if color == 'blue':\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)  # Red for blue cars\n",
    "        else:\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)  # Blue for other cars\n",
    "\n",
    "    # Process detected persons\n",
    "    for (x, y, w, h) in persons:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green for people\n",
    "    \n",
    "    return image, len(cars), len(persons)\n",
    "\n",
    "def open_image():\n",
    "    global loaded_img\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        img = cv2.imread(file_path)\n",
    "        loaded_img = img.copy()  # Save the original image for later processing\n",
    "        resized_img = resize_image(img, MAX_WINDOW_WIDTH, MAX_WINDOW_HEIGHT)\n",
    "        \n",
    "        img_rgb = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_tk = ImageTk.PhotoImage(img_pil)\n",
    "        \n",
    "        panel.config(image=img_tk)\n",
    "        panel.image = img_tk\n",
    "\n",
    "def run_detection():\n",
    "    global loaded_img\n",
    "    if loaded_img is not None:\n",
    "        resized_img = resize_image(loaded_img, MAX_WINDOW_WIDTH, MAX_WINDOW_HEIGHT)\n",
    "        processed_img, car_count, person_count = detect_cars_and_people(resized_img)\n",
    "        \n",
    "        img_rgb = cv2.cvtColor(processed_img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        img_tk = ImageTk.PhotoImage(img_pil)\n",
    "        \n",
    "        # Display updated image with bounding boxes\n",
    "        panel.config(image=img_tk)\n",
    "        panel.image = img_tk\n",
    "\n",
    "        result_text.set(f\"Cars detected: {car_count}, People detected: {person_count}\")\n",
    "    else:\n",
    "        result_text.set(\"Please upload an image first.\")\n",
    "\n",
    "# Create the GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Car Color Detection\")\n",
    "\n",
    "# Set a fixed window size\n",
    "root.geometry(f\"{MAX_WINDOW_WIDTH}x{MAX_WINDOW_HEIGHT}\")\n",
    "\n",
    "# Panel to display the image\n",
    "panel = tk.Label(root)\n",
    "panel.pack()\n",
    "\n",
    "# Button to upload an image\n",
    "upload_button = tk.Button(root, text=\"Upload Image\", command=open_image)\n",
    "upload_button.pack()\n",
    "\n",
    "# Button to run the detection\n",
    "run_button = tk.Button(root, text=\"Run Detection\", command=run_detection)\n",
    "run_button.pack()\n",
    "\n",
    "# Label to display the detection results\n",
    "result_text = tk.StringVar()\n",
    "result_label = tk.Label(root, textvariable=result_text)\n",
    "result_label.pack()\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
